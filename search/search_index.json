{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Polars Time Series Extension","text":"<p>Welcome to the documentation for the Polars Time Series Extension.</p> <p>Documentation: https://drumtorben.github.io/polars-ts</p> <p>Source Code: https://github.com/drumtorben/polars-ts</p>"},{"location":"#overview","title":"\ud83d\udcd6 Overview","text":"<p>The Polars Time Series Extention offers a wide range of metrics, feature extractors, and various tools for time series forecasting.</p>"},{"location":"#installation","title":"Installation","text":"uvpippoetry <p><code>uv add polars-timeseries</code></p> <p><code>pip install polars-timeseries</code></p> <p><code>poetry add polars-timeseries</code></p>"},{"location":"#how-to-use","title":"How to use","text":"<p>The <code>polars-ts</code> plugin is available under the namespace <code>pts</code>. See the following example where we compute the Kaboudan metric:</p> <pre><code>import polars as pl\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoETS, OptimizedTheta\n\nimport polars_ts as pts  # noqa\n\n# Create sample dataframe with columns `unique_id`, `ds`, and `y`.\ndf = (\n    pl.scan_parquet(\"https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet\")\n    .filter(pl.col(\"unique_id\").is_in([\"H1\", \"H2\", \"H3\"]))\n    .collect()\n)\n\n# Define models\nseason_length = 24\nmodels = [\n    OptimizedTheta(season_length=season_length, decomposition_type=\"additive\"),\n    AutoETS(season_length=season_length),\n]\nsf = StatsForecast(models=models, freq=1, n_jobs=-1)\n\n# Compute the Kaboudan metric in the `ts` namespace\nres = df.pts.kaboudan(sf, block_size=200, backtesting_start=0.5, n_folds=10)\n</code></pre>"},{"location":"contributing/","title":"\ud83c\udf89 Contributing","text":"<ol> <li>Fork the repository.</li> <li>Create a new feature branch.</li> <li>Submit a pull request with your changes.</li> </ol>"},{"location":"reference/polars_ts/__init__/","title":"Polars ts","text":""},{"location":"reference/polars_ts/__init__/#polars_ts","title":"<code>polars_ts</code>","text":""},{"location":"reference/polars_ts/__init__/#polars_ts.mann_kendall","title":"<code>mann_kendall(expr)</code>","text":"<p>Mann-Kendall test for expression.</p> Source code in <code>polars_ts/__init__.py</code> <pre><code>def mann_kendall(expr: IntoExpr) -&gt; pl.Expr:\n    \"\"\"Mann-Kendall test for expression.\"\"\"\n    return register_plugin_function(\n        plugin_path=PLUGIN_PATH,\n        function_name=\"mann_kendall\",\n        args=expr,\n        is_elementwise=False,\n    )\n</code></pre>"},{"location":"reference/polars_ts/main/","title":"Main","text":""},{"location":"reference/polars_ts/main/#polars_ts.main","title":"<code>polars_ts.main</code>","text":""},{"location":"reference/polars_ts/main/#polars_ts.main.main","title":"<code>main()</code>","text":"<p>Test docstring.</p> Source code in <code>polars_ts/main.py</code> <pre><code>def main():\n    \"\"\"Test docstring.\"\"\"\n    print(\"hello world!\")\n</code></pre>"},{"location":"reference/polars_ts/decomposition/__init__/","title":"Decomposition","text":""},{"location":"reference/polars_ts/decomposition/__init__/#polars_ts.decomposition","title":"<code>polars_ts.decomposition</code>","text":""},{"location":"reference/polars_ts/decomposition/seasonal_decomposition/","title":"Seasonal decomposition","text":""},{"location":"reference/polars_ts/decomposition/seasonal_decomposition/#polars_ts.decomposition.seasonal_decomposition","title":"<code>polars_ts.decomposition.seasonal_decomposition</code>","text":""},{"location":"reference/polars_ts/decomposition/seasonal_decomposition/#polars_ts.decomposition.seasonal_decomposition.seasonal_decomposition","title":"<code>seasonal_decomposition(df, freq, method='additive', id_col='unique_id', target_col='y', time_col='ds')</code>","text":"<p>Perform seasonal decomposition of time series data using either an additive or multiplicative method.</p> <p>Additive: <code>Y(t) = T(t) + S(t) + E(t)</code> Multiplicative: <code>Y(t) = T(t) * S(t) * E(t)</code></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars DataFrame containing the time series data.</p> required <code>freq</code> <code>int</code> <p>The seasonal period (e.g., 12 for monthly data with yearly seasonality).</p> required <code>method</code> <code>Literal['additive', 'multiplicative']</code> <p>The decomposition method (additive or 'multiplicative').</p> <code>'additive'</code> <code>id_col</code> <code>str</code> <p>The column to group by (e.g., for multiple time series).</p> <code>'unique_id'</code> <code>target_col</code> <code>str</code> <p>The column containing the time series values to decompose.</p> <code>'y'</code> <code>time_col</code> <code>str</code> <p>The column containing the time values.</p> <code>'ds'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame with the decomposed components: trend, seasonal component, and residuals.</p> Source code in <code>polars_ts/decomposition/seasonal_decomposition.py</code> <pre><code>def seasonal_decomposition(\n    df: pl.DataFrame,\n    freq: int,\n    method: Literal[\"additive\", \"multiplicative\"] = \"additive\",\n    id_col: str = \"unique_id\",\n    target_col: str = \"y\",\n    time_col: str = \"ds\",\n) -&gt; pl.DataFrame:\n    \"\"\"Perform seasonal decomposition of time series data using either an additive or multiplicative method.\n\n    Additive: `Y(t) = T(t) + S(t) + E(t)`  \n    Multiplicative: `Y(t) = T(t) * S(t) * E(t)`\n\n    Args:\n        df: Polars DataFrame containing the time series data.\n        freq: The seasonal period (e.g., 12 for monthly data with yearly seasonality).\n        method: The decomposition method (additive or 'multiplicative').\n        id_col: The column to group by (e.g., for multiple time series).\n        target_col: The column containing the time series values to decompose.\n        time_col: The column containing the time values.\n\n    Returns:\n        Polars DataFrame with the decomposed components: trend, seasonal component, and residuals.\n\n    \"\"\"\n    period_idx = pl.col(time_col).cum_count().mod(freq).over(id_col).alias(\"period_idx\")\n\n    # Trend: Rolling mean with window size = freq\n    trend_expr = pl.col(target_col).rolling_mean(window_size=freq, center=True).over(id_col).alias(\"trend\")\n\n    if method == \"additive\":\n        func = pl.Expr.sub\n    elif method == \"multiplicative\":\n        func = pl.Expr.truediv\n\n    # Seasonal component (additive method)\n    seasonal_component_expr = (\n        pl.col(target_col).pipe(func, \"trend\").mean().over(id_col, \"period_idx\").alias(\"seasonal_idx\")\n    )\n\n    # Adjust seasonal component to have mean = 0 (for additive)\n    seasonal_idx_expr = pl.col(\"seasonal_idx\").sub(pl.col(\"seasonal_idx\").mean().over(id_col)).alias(f\"seasonal_{freq}\")\n\n    # Residuals:\n    # Original series - trend - seasonal components (additive)\n    # Original series / trend / seasonal components (multiplicative)\n    residuals_expr = pl.col(target_col).pipe(func, pl.col(\"trend\")).pipe(func, pl.col(f\"seasonal_{freq}\"))\n\n    df = (\n        df.with_columns(period_idx, trend_expr)\n        .with_columns(seasonal_component_expr)\n        .with_columns(seasonal_idx_expr)\n        .with_columns(residuals_expr.alias(\"resid\"))\n        .drop(\"period_idx\", \"seasonal_idx\")\n        # drop nulls created by centered moving average\n        .drop_nulls()\n    )\n\n    return df\n</code></pre>"},{"location":"reference/polars_ts/metrics/__init__/","title":"Metrics","text":""},{"location":"reference/polars_ts/metrics/__init__/#polars_ts.metrics","title":"<code>polars_ts.metrics</code>","text":""},{"location":"reference/polars_ts/metrics/kaboudan/","title":"Kaboudan","text":""},{"location":"reference/polars_ts/metrics/kaboudan/#polars_ts.metrics.kaboudan","title":"<code>polars_ts.metrics.kaboudan</code>","text":"<p>Kaboudan Metrics Module.</p> <p>Provides the <code>Kaboudan</code> class for computing Kaboudan and modified Kaboudan metrics to evaluate time series forecasting models using backtesting and block shuffling techniques.</p>"},{"location":"reference/polars_ts/metrics/kaboudan/#polars_ts.metrics.kaboudan.Kaboudan","title":"<code>Kaboudan</code>  <code>dataclass</code>","text":"<p>A class for computing the Kaboudan and modified Kaboudan metrics.</p> <p>It uses StatsForecast for backtesting and block shuffling operations to measure model performance under controlled perturbations.</p> <p>Attributes:</p> Name Type Description <code>sf</code> <code>StatsForecast</code> <p>StatsForecast instance for model training and evaluation.</p> <code>backtesting_start</code> <code>float</code> <p>Fraction of the data used as the initial training set.</p> <code>n_folds</code> <code>int</code> <p>Number of backtesting folds (rolling-origin windows).</p> <code>block_size</code> <code>int</code> <p>Size of each block used during block-based shuffling.</p> <code>seed</code> <code>int</code> <p>Random seed for reproducible shuffling. Defaults to 42.</p> <code>id_col</code> <code>str</code> <p>Name of the column identifying each time series group. Defaults to <code>unique_id</code>.</p> <code>time_col</code> <code>str</code> <p>Name of the column representing the chronological axis. Defaults to <code>ds</code>.</p> <code>value_col</code> <code>str</code> <p>Name of the column representing the target variable. Defaults to <code>y</code>.</p> <code>modified</code> <code>bool</code> <p>Whether to use the modified Kaboudan metric, which applies clipping to zero. Defaults to <code>True</code>.</p> <code>agg</code> <code>bool</code> <p>Whether to average the metrics over all the individual time series or not. Defaults to <code>True</code>.</p> Source code in <code>polars_ts/metrics/kaboudan.py</code> <pre><code>@dataclass\nclass Kaboudan:\n    \"\"\"A class for computing the Kaboudan and modified Kaboudan metrics.\n\n    It uses StatsForecast for backtesting and block shuffling operations\n    to measure model performance under controlled perturbations.\n\n    Attributes:\n        sf: StatsForecast instance for model training and evaluation.\n        backtesting_start: Fraction of the data used as the initial training set.\n        n_folds: Number of backtesting folds (rolling-origin windows).\n        block_size: Size of each block used during block-based shuffling.\n        seed: Random seed for reproducible shuffling. Defaults to 42.\n        id_col: Name of the column identifying each time series group. Defaults to `unique_id`.\n        time_col: Name of the column representing the chronological axis. Defaults to `ds`.\n        value_col: Name of the column representing the target variable. Defaults to `y`.\n        modified: Whether to use the modified Kaboudan metric, which applies clipping to zero. Defaults to `True`.\n        agg: Whether to average the metrics over all the individual time series or not. Defaults to `True`.\n\n    \"\"\"\n\n    sf: StatsForecast\n    backtesting_start: float\n    n_folds: int\n    block_size: int\n    seed: int = 42\n    id_col: str = \"unique_id\"\n    time_col: str = \"ds\"\n    value_col: str = \"y\"\n    modified: bool = True\n    agg: bool = False\n\n    def block_shuffle_by_id(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"Randomly shuffles rows in fixed-size blocks within each group identified by `id_col`.\n\n        This method sorts the data by `id_col` and then by `time_col`. For each group:  \n\n        1. A zero-based row index (`__row_in_group`) is assigned using `cum_count()`.\n        2. The method determines the number of blocks (`num_blocks`) by dividing the number of\n        rows in the first group by `self.block_size` and forcing at least one block.\n        3. Each row is assigned a `__chunk_id` based on integer division of `__row_in_group` by `num_blocks`.\n        4. The DataFrame is then partitioned by both `id_col` and `__chunk_id`, producing blocks.\n        5. These blocks are randomly shuffled, concatenated, and finally re-sorted by `id_col`\n        and `time_col` within each group.\n\n        Args:\n            df: A Polars DataFrame containing at least `id_col`, `time_col`, and `value_col`.\n\n        Returns:\n            A new DataFrame in which each group's rows are rearranged by randomly shuffling \\\n            the entire blocks. The shuffle is reproducible if a seed is set (`self.seed`).\n\n        \"\"\"\n        num_blocks = max(df.filter(pl.col(self.id_col) == pl.first(self.id_col)).height // self.block_size, 1)\n        dfs = (\n            df.sort(self.id_col, self.time_col)\n            .with_columns(\n                # cum_count() over each group to get the row index within that group\n                pl.col(self.value_col).cum_count().over(self.id_col).alias(\"__row_in_group\")\n            )\n            .with_columns(\n                # __chunk_id = row_in_group // block_size\n                (pl.col(\"__row_in_group\") // num_blocks).alias(\"__chunk_id\"),\n            )\n            .partition_by(self.id_col, \"__chunk_id\")\n        )\n        if self.seed is not None:\n            # set seed\n            random.seed(self.seed)\n        random.shuffle(dfs)\n        df = (\n            pl.concat(dfs)\n            .drop(\"__row_in_group\", \"__chunk_id\")\n            .sort(self.id_col)\n            .with_columns(pl.col(self.time_col).sort().over(self.id_col))\n        )\n        return df\n\n    def split_in_blocks_by_id(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"Split each group's time series into `n_folds` sequential blocks.\n\n        First, the DataFrame is sorted `by id_col` and `time_col`. Then, for each group (identified\n        by `id_col`), a zero-based row index is assigned in `row_index`. Finally, `block` is\n        computed by scaling `row_index` by the ratio `(n_folds / group_size)` for that group,\n        flooring the result, and shifting by 1 to make blocks range from 1 to `n_folds`.\n\n        Args:\n            df: A DataFrame containing columns matching `id_col`, `time_col`, and `value_col`.\n\n        Returns:\n            A new DataFrame with one additional `block` column.\n\n        \"\"\"\n        df = (\n            df.sort(self.id_col, self.time_col)\n            # Assign a zero-based row index per group\n            .with_columns((pl.col(self.value_col).cum_count() - 1).over(self.id_col).alias(\"__row_index\"))\n            # Convert row_index to block assignments [1..n_folds]\n            # block = floor(row_index * n_folds / group_size) + 1\n            # where group_size is (max row_index in the group + 1).\n            .with_columns(\n                (((pl.col(\"__row_index\") * self.n_folds) / pl.len()).over(self.id_col).floor() + 1)\n                .cast(pl.Int32)\n                .alias(\"block\")\n            )\n            .drop(\"__row_index\")\n        )\n        return df\n\n    def backtest(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"Perform rolling-origin backtesting on the provided DataFrame using cross-validation.\n\n        This method implements a multi-step cross-validation approach by:\n\n        1. Computing the minimal series length among all groups in the DataFrame.\n        2. Determining the initial training length (`history_len`) as `backtesting_start * min_len`,\n        and setting the test length (`test_len`) as the remainder.\n        3. Dividing the test portion into `n_folds` sequential segments. Each segment length\n        determines the forecast horizon (`h`) and `step_size`.\n        4. Calling StatsForecast's `cross_validation()` method with `h` and `step_size` both equal to\n        the segment length.\n\n        Args:\n            df: A Polars DataFrame that must contain at least the columns `id_col`, `time_col`, and `value_col`.\n\n        Returns:\n            A Polars DataFrame (or Series) of root mean squared error (RMSE) values, averaged across \\\n            the rolling-origin folds for each model. Columns represent different models.\n\n        \"\"\"\n        # 1) Compute minimum series length across groups (if multiple series).\n        size_df = df.group_by(self.id_col).agg(pl.count(self.time_col).alias(\"series_length\"))\n        min_len = size_df[\"series_length\"].min()\n\n        # 2) Derive training/test sizes based on `backtesting_start`\n        history_len = int(min_len * self.backtesting_start)  # length of initial training portion\n        test_len = min_len - history_len  # length of the test portion\n\n        # 3) Split test portion into n_folds =&gt; block_len\n        block_len = max(test_len // self.n_folds, 1)\n\n        # 4) Our horizon (h) and step_size both match the block length\n        h = block_len\n        step_size = block_len\n\n        # 5) Call cross_validation with these derived parameters\n        cv_df = self.sf.cross_validation(df=df, h=h, step_size=step_size, n_windows=self.n_folds)\n\n        # Compute SSE (or RMSE, etc.)\n        model_names = [m.alias for m in self.sf.models]\n        errors = losses.rmse(cv_df, models=model_names, target_col=self.value_col)\n        return errors\n\n    def kaboudan_metric(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"Compute the Kaboudan Metric by comparing model errors before and after block-based shuffling.\n\n        This method first calculates a baseline error using `backtest`. Then it applies\n        `block_shuffle_by_id` to shuffle each group's rows, re-performs `backtest` on the shuffled data,\n        and compares the two sets of errors. The final metric indicates how much performance\n        degrades due to the block shuffle.\n\n        Steps:\n\n        1. Compute the baseline RMSE (`sse_before`) for the unshuffled data.\n        2. Shuffle the data in blocks (`block_shuffle_by_id`).\n        3. Compute the RMSE (`sse_after`) of the shuffled data.\n        4. Compute the ratio `sse_before / sse_after` and transform it by `(1 - sqrt(ratio))`.\n\n        If `modified` is True, the resulting metric is clipped at 0 to avoid negative values.\n\n        Args:\n            df: A Polars DataFrame with columns for `id_col`, `time_col`, and `value_col`.\n\n        Returns:\n            A Polars DataFrame containing columns of Kaboudan Metric values for each model. \\\n            If `modified` is True, negative values are clipped to zero.\n\n        \"\"\"\n        sse_before = self.backtest(df)\n\n        df_shuffled = self.block_shuffle_by_id(df)\n        sse_after = self.backtest(df_shuffled)\n\n        # Compute the metric\n        scores = (sse_before.drop(self.id_col) / sse_after.drop(self.id_col)).select(\n            sse_before[self.id_col],\n            (1 - pl.all().sqrt()).name.keep(),\n        )\n\n        if self.agg:\n            scores = scores.drop(self.id_col).mean()\n        if self.modified:\n            # clip to zero\n            return scores.with_columns(pl.exclude(self.id_col).clip(lower_bound=0))\n        else:\n            return scores\n</code></pre>"},{"location":"reference/polars_ts/metrics/kaboudan/#polars_ts.metrics.kaboudan.Kaboudan.block_shuffle_by_id","title":"<code>block_shuffle_by_id(df)</code>","text":"<p>Randomly shuffles rows in fixed-size blocks within each group identified by <code>id_col</code>.</p> <p>This method sorts the data by <code>id_col</code> and then by <code>time_col</code>. For each group:  </p> <ol> <li>A zero-based row index (<code>__row_in_group</code>) is assigned using <code>cum_count()</code>.</li> <li>The method determines the number of blocks (<code>num_blocks</code>) by dividing the number of rows in the first group by <code>self.block_size</code> and forcing at least one block.</li> <li>Each row is assigned a <code>__chunk_id</code> based on integer division of <code>__row_in_group</code> by <code>num_blocks</code>.</li> <li>The DataFrame is then partitioned by both <code>id_col</code> and <code>__chunk_id</code>, producing blocks.</li> <li>These blocks are randomly shuffled, concatenated, and finally re-sorted by <code>id_col</code> and <code>time_col</code> within each group.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Polars DataFrame containing at least <code>id_col</code>, <code>time_col</code>, and <code>value_col</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which each group's rows are rearranged by randomly shuffling             the entire blocks. The shuffle is reproducible if a seed is set (<code>self.seed</code>).</p> Source code in <code>polars_ts/metrics/kaboudan.py</code> <pre><code>def block_shuffle_by_id(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Randomly shuffles rows in fixed-size blocks within each group identified by `id_col`.\n\n    This method sorts the data by `id_col` and then by `time_col`. For each group:  \n\n    1. A zero-based row index (`__row_in_group`) is assigned using `cum_count()`.\n    2. The method determines the number of blocks (`num_blocks`) by dividing the number of\n    rows in the first group by `self.block_size` and forcing at least one block.\n    3. Each row is assigned a `__chunk_id` based on integer division of `__row_in_group` by `num_blocks`.\n    4. The DataFrame is then partitioned by both `id_col` and `__chunk_id`, producing blocks.\n    5. These blocks are randomly shuffled, concatenated, and finally re-sorted by `id_col`\n    and `time_col` within each group.\n\n    Args:\n        df: A Polars DataFrame containing at least `id_col`, `time_col`, and `value_col`.\n\n    Returns:\n        A new DataFrame in which each group's rows are rearranged by randomly shuffling \\\n        the entire blocks. The shuffle is reproducible if a seed is set (`self.seed`).\n\n    \"\"\"\n    num_blocks = max(df.filter(pl.col(self.id_col) == pl.first(self.id_col)).height // self.block_size, 1)\n    dfs = (\n        df.sort(self.id_col, self.time_col)\n        .with_columns(\n            # cum_count() over each group to get the row index within that group\n            pl.col(self.value_col).cum_count().over(self.id_col).alias(\"__row_in_group\")\n        )\n        .with_columns(\n            # __chunk_id = row_in_group // block_size\n            (pl.col(\"__row_in_group\") // num_blocks).alias(\"__chunk_id\"),\n        )\n        .partition_by(self.id_col, \"__chunk_id\")\n    )\n    if self.seed is not None:\n        # set seed\n        random.seed(self.seed)\n    random.shuffle(dfs)\n    df = (\n        pl.concat(dfs)\n        .drop(\"__row_in_group\", \"__chunk_id\")\n        .sort(self.id_col)\n        .with_columns(pl.col(self.time_col).sort().over(self.id_col))\n    )\n    return df\n</code></pre>"},{"location":"reference/polars_ts/metrics/kaboudan/#polars_ts.metrics.kaboudan.Kaboudan.split_in_blocks_by_id","title":"<code>split_in_blocks_by_id(df)</code>","text":"<p>Split each group's time series into <code>n_folds</code> sequential blocks.</p> <p>First, the DataFrame is sorted <code>by id_col</code> and <code>time_col</code>. Then, for each group (identified by <code>id_col</code>), a zero-based row index is assigned in <code>row_index</code>. Finally, <code>block</code> is computed by scaling <code>row_index</code> by the ratio <code>(n_folds / group_size)</code> for that group, flooring the result, and shifting by 1 to make blocks range from 1 to <code>n_folds</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing columns matching <code>id_col</code>, <code>time_col</code>, and <code>value_col</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with one additional <code>block</code> column.</p> Source code in <code>polars_ts/metrics/kaboudan.py</code> <pre><code>def split_in_blocks_by_id(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Split each group's time series into `n_folds` sequential blocks.\n\n    First, the DataFrame is sorted `by id_col` and `time_col`. Then, for each group (identified\n    by `id_col`), a zero-based row index is assigned in `row_index`. Finally, `block` is\n    computed by scaling `row_index` by the ratio `(n_folds / group_size)` for that group,\n    flooring the result, and shifting by 1 to make blocks range from 1 to `n_folds`.\n\n    Args:\n        df: A DataFrame containing columns matching `id_col`, `time_col`, and `value_col`.\n\n    Returns:\n        A new DataFrame with one additional `block` column.\n\n    \"\"\"\n    df = (\n        df.sort(self.id_col, self.time_col)\n        # Assign a zero-based row index per group\n        .with_columns((pl.col(self.value_col).cum_count() - 1).over(self.id_col).alias(\"__row_index\"))\n        # Convert row_index to block assignments [1..n_folds]\n        # block = floor(row_index * n_folds / group_size) + 1\n        # where group_size is (max row_index in the group + 1).\n        .with_columns(\n            (((pl.col(\"__row_index\") * self.n_folds) / pl.len()).over(self.id_col).floor() + 1)\n            .cast(pl.Int32)\n            .alias(\"block\")\n        )\n        .drop(\"__row_index\")\n    )\n    return df\n</code></pre>"},{"location":"reference/polars_ts/metrics/kaboudan/#polars_ts.metrics.kaboudan.Kaboudan.backtest","title":"<code>backtest(df)</code>","text":"<p>Perform rolling-origin backtesting on the provided DataFrame using cross-validation.</p> <p>This method implements a multi-step cross-validation approach by:</p> <ol> <li>Computing the minimal series length among all groups in the DataFrame.</li> <li>Determining the initial training length (<code>history_len</code>) as <code>backtesting_start * min_len</code>, and setting the test length (<code>test_len</code>) as the remainder.</li> <li>Dividing the test portion into <code>n_folds</code> sequential segments. Each segment length determines the forecast horizon (<code>h</code>) and <code>step_size</code>.</li> <li>Calling StatsForecast's <code>cross_validation()</code> method with <code>h</code> and <code>step_size</code> both equal to the segment length.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Polars DataFrame that must contain at least the columns <code>id_col</code>, <code>time_col</code>, and <code>value_col</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame (or Series) of root mean squared error (RMSE) values, averaged across             the rolling-origin folds for each model. Columns represent different models.</p> Source code in <code>polars_ts/metrics/kaboudan.py</code> <pre><code>def backtest(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Perform rolling-origin backtesting on the provided DataFrame using cross-validation.\n\n    This method implements a multi-step cross-validation approach by:\n\n    1. Computing the minimal series length among all groups in the DataFrame.\n    2. Determining the initial training length (`history_len`) as `backtesting_start * min_len`,\n    and setting the test length (`test_len`) as the remainder.\n    3. Dividing the test portion into `n_folds` sequential segments. Each segment length\n    determines the forecast horizon (`h`) and `step_size`.\n    4. Calling StatsForecast's `cross_validation()` method with `h` and `step_size` both equal to\n    the segment length.\n\n    Args:\n        df: A Polars DataFrame that must contain at least the columns `id_col`, `time_col`, and `value_col`.\n\n    Returns:\n        A Polars DataFrame (or Series) of root mean squared error (RMSE) values, averaged across \\\n        the rolling-origin folds for each model. Columns represent different models.\n\n    \"\"\"\n    # 1) Compute minimum series length across groups (if multiple series).\n    size_df = df.group_by(self.id_col).agg(pl.count(self.time_col).alias(\"series_length\"))\n    min_len = size_df[\"series_length\"].min()\n\n    # 2) Derive training/test sizes based on `backtesting_start`\n    history_len = int(min_len * self.backtesting_start)  # length of initial training portion\n    test_len = min_len - history_len  # length of the test portion\n\n    # 3) Split test portion into n_folds =&gt; block_len\n    block_len = max(test_len // self.n_folds, 1)\n\n    # 4) Our horizon (h) and step_size both match the block length\n    h = block_len\n    step_size = block_len\n\n    # 5) Call cross_validation with these derived parameters\n    cv_df = self.sf.cross_validation(df=df, h=h, step_size=step_size, n_windows=self.n_folds)\n\n    # Compute SSE (or RMSE, etc.)\n    model_names = [m.alias for m in self.sf.models]\n    errors = losses.rmse(cv_df, models=model_names, target_col=self.value_col)\n    return errors\n</code></pre>"},{"location":"reference/polars_ts/metrics/kaboudan/#polars_ts.metrics.kaboudan.Kaboudan.kaboudan_metric","title":"<code>kaboudan_metric(df)</code>","text":"<p>Compute the Kaboudan Metric by comparing model errors before and after block-based shuffling.</p> <p>This method first calculates a baseline error using <code>backtest</code>. Then it applies <code>block_shuffle_by_id</code> to shuffle each group's rows, re-performs <code>backtest</code> on the shuffled data, and compares the two sets of errors. The final metric indicates how much performance degrades due to the block shuffle.</p> <p>Steps:</p> <ol> <li>Compute the baseline RMSE (<code>sse_before</code>) for the unshuffled data.</li> <li>Shuffle the data in blocks (<code>block_shuffle_by_id</code>).</li> <li>Compute the RMSE (<code>sse_after</code>) of the shuffled data.</li> <li>Compute the ratio <code>sse_before / sse_after</code> and transform it by <code>(1 - sqrt(ratio))</code>.</li> </ol> <p>If <code>modified</code> is True, the resulting metric is clipped at 0 to avoid negative values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Polars DataFrame with columns for <code>id_col</code>, <code>time_col</code>, and <code>value_col</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame containing columns of Kaboudan Metric values for each model.             If <code>modified</code> is True, negative values are clipped to zero.</p> Source code in <code>polars_ts/metrics/kaboudan.py</code> <pre><code>def kaboudan_metric(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute the Kaboudan Metric by comparing model errors before and after block-based shuffling.\n\n    This method first calculates a baseline error using `backtest`. Then it applies\n    `block_shuffle_by_id` to shuffle each group's rows, re-performs `backtest` on the shuffled data,\n    and compares the two sets of errors. The final metric indicates how much performance\n    degrades due to the block shuffle.\n\n    Steps:\n\n    1. Compute the baseline RMSE (`sse_before`) for the unshuffled data.\n    2. Shuffle the data in blocks (`block_shuffle_by_id`).\n    3. Compute the RMSE (`sse_after`) of the shuffled data.\n    4. Compute the ratio `sse_before / sse_after` and transform it by `(1 - sqrt(ratio))`.\n\n    If `modified` is True, the resulting metric is clipped at 0 to avoid negative values.\n\n    Args:\n        df: A Polars DataFrame with columns for `id_col`, `time_col`, and `value_col`.\n\n    Returns:\n        A Polars DataFrame containing columns of Kaboudan Metric values for each model. \\\n        If `modified` is True, negative values are clipped to zero.\n\n    \"\"\"\n    sse_before = self.backtest(df)\n\n    df_shuffled = self.block_shuffle_by_id(df)\n    sse_after = self.backtest(df_shuffled)\n\n    # Compute the metric\n    scores = (sse_before.drop(self.id_col) / sse_after.drop(self.id_col)).select(\n        sse_before[self.id_col],\n        (1 - pl.all().sqrt()).name.keep(),\n    )\n\n    if self.agg:\n        scores = scores.drop(self.id_col).mean()\n    if self.modified:\n        # clip to zero\n        return scores.with_columns(pl.exclude(self.id_col).clip(lower_bound=0))\n    else:\n        return scores\n</code></pre>"}]}